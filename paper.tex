\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}
\usepackage[small]{caption}
\usepackage{color}

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}





\title{Field Recordings, Mind, and the Eternal Hum or Whisper: Observing neural activity out of the lab and at scale}

\author{\authorname{Nick Merrill\sup{1}, Thomas Maillart\sup{1} and John Chuang\sup{1}}
\affiliation{\sup{1}School of Information, UC Berkeley, California, USA}
\email{ffff@berkeley.edu}
}

\keywords{?}




\abstract{Though our minds are continuously outputting a wealth of neurological data as we live and interact with people and our environment, the vast majority of our knowledge about the mind is inferred from observations made in controlled, laboratory conditions. Recently, the equipment used for brain-computer interface (BCI) has become portable, powerful and ergonomic enough that the continuous recording of neural signals ``in the wild'' is a present reality. This paper discusses the design space of a system for the pervasive recording of neural signals in real-world settings. We propose a decentralized repository aimed at at-scale data anlytics on ``in the wild'' EEG data and enumerate possibilities for how such data might be efficiently stored and how the privacy of research participants could be protected. we believe this design space might be relevant to sensor-driven applications generally.}

\onecolumn \maketitle \normalsize \vfill




\section{Introduction}

Our ability to build effective, useful brain-computer interface (BCI) is limited by the capabilities of our scanning devices and by our inchoate knowledge of neural activity, especially about how it looks outside of lab settings. While we expect mobile brain scanners to improve dramatically over the next few years, our ability to understand brains ``in the wild'' is hampered primarily by a paucity of relevant data. Almost all neurological studies, BCI research included, occurs under controlled conditions rather than in the normal human habitat, a perceptually rich environment in which people are moving freely and interacting with space, objects and people.
% consider adding: (the Melon headband, Interaxon Muse and Emotiv Insight all promise much better scanning resolution, comfort and battery life than current-generation devices)

Presently, there exists no large-scale repository for ``field recordings'' (i.e., long-timescale recordings taken in uncontrolled, non-lab environments) of neural signals, making it difficult for researchers to draw the sorts of observations that might improve BCIs, and our understanding of mind generally. However, the equipment used by BCI researchers is now small and ergonomic enough as to allow chronic \& ubiquituous recording of neural signals.

The difficulty of dealing with subject-to-subject and trial-to-trial changes in neural signals provides a salient example of our knowledge gap: a BCI that works with one person will most likely not work with another, and may even stop working on the original subject over time, due simply to the fact that minds shift and change and grow. It is tempting to believe that such changes, especially those observed within subjects over time, can be anticipated or at least described statistically, to some extent, given enough data. However, without a large-scale source of neural field recordings, it is difficult or impossible for researchers to make the sorts of observations necessary to investigate such patterns.

This paper imagines a platform for neural field recordings, approaching the problem from an information architecture standpoint and from a user interface one. How do we scale a data collection platform to hundreds of thousands of users, all of whom are producing megabytes of data every minute? How do we incentivize users to donate their neural data, and how do we assure their privacy? How might we manage access and ownership in a publically available research repository while maintaining the confidentiality of individuals involved?

*maybe we talk about what this paper covers here, in summary, when we figure that out*






\section{Background \& Related Work}

\subsection{Brain-computer interface}

Brain-computer interface aims to establish a direct connection between the human nervous system and a digital computer, without intermediate muscular intervention. Obvious applications include devices and tools for the severely disabled, for whom muscular activity is difficult or impossible, but BCI has a potentially very wide audience if it is convenient enough to use; it promises to increase our overall communicative capacity with digital machines, and to passively monitor our cognitive load and emotional states, helping us better adapt to our environment and to ourselves.

Attempts at BCI generally employ adaptive, machine-learning algorithms, as neural signals vary from person to person, and change within individuals over time (this is refferred to as ``nonstationarity'' - that is,  that there exists no static model of neural functioning that will work reliably with all people at all times). Some \textit{a priori} neurological knowledge is generally employed, usually around a central strategy (e.g., P300, SSVP, motor imagery, etc; see [] for a review of BCI strategies). Thereafter, an adaptive algorithm, usually a linear classifier, is trained on sample data, and this calibration closes the gap between the theoretical basis behind the system and the observed data in reality.

\subsection{BCI in the wild}

There have been numerous attempts to deploy brain-computer interfaces in outside-of-lab settings. Many wireless EEG headsets exist - there are at least four on the market presently, and three new devices slated for release in the immediate future. From an application standpoint, some commercial ventures into ``direct-control'' BCI applications have recieved wide media attention as of late (e.g. an application for Google Glass that takes a picture when the user focuses). Historically, consumer BCI applications have centered more around affective and emotional measurement such as stress detectors, meditation coaches, applications aimed at monitoring mental workload, and so on. The claims behind these devices are difficult to rigorously validate, as the developers of these applications understandably do not publish detailed information about their use.

From the academic side of things, a few applications have surfaced employing consumer-grade wireless headsets. One application authenticates users using their brainwaves alone, and various others have replicated the levels of control achieved with in-lab direct-control BCI systems, with varying levels of success. A few of these projects have cognitive load measurements in real-life work situations, and a few have looked at in-the-wild emotional response. Overwhelmingly, however, academic approaches to BCI have approached their subjects and analysis from inside the lab, while free-market ventures into BCI have not rigorously collected or disseminated data on their applications' use.


\subsection{Large-scale data repositories for scientific research}








\section{Specifications}

The BCI community, as well as the neuroscience and wearable computing \textit{(have i really established this, does this paper really establish its relevance to the larger wearables community)} communities at large, could benefit from a large-scale repository of neurological data recorded in real-world settings. In this section, we describe the specifications for an open collection platform aimed at field recordings of neural data. We focus specifically on the use of electroencephalography (EEG) \textit{(mention eeg stuff earlier?)} due to its wide adoption in BCI and as a format for consumer brainscanners.

\subsection{Scalability}

A system for storing neural recordings is unlike traditional web applications (Twitter, Facebook, Youtube) in that we expect each user to produce a much larger volume of (raw) data, and to submit that data more frequently. The Neurosky MindWave, a current-generation wireless EEG with a single electrode, produces approximately a megabyte of raw data every minute. This bitrate will increase linearly with the number of electrodes on the device; just the devices on the immediate horizon will produce up to five times this volume of data. With each user producing gigabytes of data every day in chronic recording scenarios, the system will not scale well to large number of users.

\subsection{Redundancy \& Control}

Any useful repository of scientific data must be openly accessible, but ownership over the data itself becomes a more nuanced issue as the volume of data increases. For large data repositories, the cost of maintaining the requisite server space and the bandwidth necessary to make the data on those servers widely accessible can be formidable, at least with traditional, centralized architectures. Besides, these sorts of architectures put data under the ownership of a single entity or individual, which is problematic occasionally from a practical standpoint (data loss, outages, economic issues, etc), but from an intellectual standpoint as well, as scientific data ought to be ..... well. lol

\subsection{Privacy}

In line with the regulations enforced by any internal review board, all data collected on this platform must be anonymous or, at the very least, psuedonymous. Users should be able to remove their data from the system at any time (``the right to be forgotten'' as it has lately been called, analogous to the right of experimental subjects to withdraw from a study), and there might exist an option by which users can require permission for their data to be accessed, or at least to be accessed on a continual basis.

\subsection{Keeping time}

\subsection{End user concerns}

If we are to collect intimate information from volunteers, we must certainly offer them something in return. Any system that collects data on a large scale must be equally capable of delivering value based on that data, preferably such that applications could adjust fluidly to future observations or developments based off the data collected through the system at large. 






\section{Design recommendations}

This section proposes design ideas for building a large-scale repository for EEG data that is efficient, mindful of its users' privacy, and resiliant to centralized points of failure.

\subsection{A lossy fileformat for EEG data}
Raw recordings may not be necessary to produce useful insights about neural data, especially at high volume. A lossy file format may be sufficient, the proverbial mp3 of neural data. What might this file format look like?

One option is to ``quantize'' the data - to pull points we believe to be relevant or informative and to discard the rest of the signal. One quantization method we have found particularly effective is the selection of logarithmically-spaced points in a continuous power spectrum of EEG data. We find that even at low numbers of bins (25-50, about 80 bytes), we can achieve very strong classification accuracy on mental imagery tasks using a support vector machine (SVM). Our accuracy can be bulstered by compressing continuous readings in the time dimension by averaging each bin reading over a number of signals, an effect that we have found to persit at EEG recordings up to ten seconds in length. This latter technique has the happy effect of further reducing our data output (at ten seconds, the total bin data would be 80 bytes as opposed to 80 bytes * 20 power spectra). These findings are summarized in Figure 1.

Another approach is to maximize the informativeness of the signal by replacing individual data points with a model that describes statistically how a reading appeared over a period of time. A brain with relatively stable activity (e.g. deep sleep, or REM, typified by elevated activity in the gamma band and supressed activity in the alpha band) need not be described as an hour of continuous readings; instead, it could be described as a statistical range of observed values, or even as a cyclical pattern that repeats, if such a model is relevant and appropriate. Data in this format need be represented merely by the model and whatever observed data deviated from it (e.g., ``an hour of REM with a spike in alpha activity between seconds 530 and 620''). By representing readings this way, an EEG signal could be described probabilistically at any point in time with representing discrete readings from the device. \textit{does this sort of technique have a name? i know this has to do with exploiting low entropy in the signal, so i feel it must have some kind of information theoretic way of being expressed or quantified; i wouldn't be surprised if there's a class of compression algos aimed at generating patterns like these.}

\subsection{A decentralized network}

\subsection{Digital rights management for research participants}

\subsection{A decoupled application layer}





\section{Discussion}




% \input{Sections/intro}
% \input{Sections/background}

\vfill
\bibliographystyle{apalike}
{\small
\bibliography{references}}

\vfill
\end{document}

