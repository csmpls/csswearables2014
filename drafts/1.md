
# introduction

our ability to build effective, useful brain-computer interfaces (BCthIs) is limited by the capabilities of scanning devices and by our inchoate knowledge of neural activity, especially in realistic, out-of-lab settings where subjects are moving freely, interacting with perceptually rich environments.

while we expect mobile scanning devices to improve dramatically in the near future (the Melon headband, Interaxon Muse and Emotiv Insight, to name a few examples), our ability to understand brains "in the wild" is limited primarily by a paucity of data. no large-scale repository for real-world neural recordings currently exists, making it difficult for researchers to draw observations. 

one of the most salient examples of this problem is the difficulty of dealing with nonstationarityy nature of neural signals - the fact that a BCI that works with one person will most likely not work with another, and may even stop working on the original subject over time, due simply to the fact that mind is ever-changing. it is tempting to believe that such changes, especially those observed within subjects over time, are to some degree statistically predictable given enough data. however, without a large-scale source of neural "field recordings" (i.e., long-timescale recordings taken in uncontrolled, non-lab settings), it is difficult for researchers to make the sorts of observations necessary to build effective BCIs.

this paper explores what a platform for neural field recordings might look like, approaching the problem from an informatino architecture standpoint and from a human-computer interaction one. how do we scale a data collection platform to thousands or hundreds of thousands of users, all producing megabytes of data every minute? how can we incentivize users to donate their neural data, and how do we protect their privacy? how might we manage access and ownership to a publically available research repository?

*maybe we talk about what this paper covers here, in summary, when we figure that out*



- that it scale
- that it be publically available, i.e. queryable by all
- preferably, that not be owned by any one person
- preferably, that users can revoke access to their things


motivations are mostly that we're still so bad at this stuff, still so bad at knowing, evidence is that glass app that just came out. and the consistently low bitrates of BCIs. we need more data.

but EEG data is megabytes per minute - possibly for days and days. we need the "mp3" of brainwave data. a lossy but acceptable format that allows these data to be easily shared. 

## topics 

### - what that file format might look like ?

big idea here is 'replacing the db with the model'

well, compression, but what does compression mean here? what are "the interesting bits"? does it all depend on the model? or is there some generalizeable way of finding information, a la doing "diffs" on power frequency data? like, wow that frequency bin really peaked on that second, while the rest of those bins hovered around where they were?


### - what would the format on which it's collected look like?

distributed has its advantages!

but privacy is important too!

### - how do we deliver value to people who use this service?

users need to have incentive - something in it for the

now, this collection service can exist separately from the application layer, but it ought to be possible for the application and collection layers to communicate with each other

here we can talk about "delivering a classifier"







 (1) it's not owned by someone and (2) people can revoke access to their data 

tension btwn 1 and 2



ships you back some ML metric based on cloud........constantly refactoring metric │07-21 23:24 --> going back to this idea of classifier instead of a database. │07-21 23:24 --> the cloud just ships you a classifier, rather than the data used to create it.

                      │07-21 23:25 <== you need to collect some of the data but you never need to SEND data




a system by which you need a person's "permission" to unlock the data?


I think the concept of
                       │                minimizing what is in the
                       │                database to the "interesting"
                       │                bits would be dramatically
                       │                helpful for compression
                       │07-21 23:38 --> agreed.
    7-21 23:38 <== upstream, no data matters except the data that doesn't match the



so.........what's the model? does it all depend on the model we're using? or is there some generalizeable way of finding information, a la doing "diffs" on power frequency data? like, wow that frequency bin really peaked on that second, while the rest of those bins hovered around where they were?





take the fully academic approach


        - distributed data stores

        - stuff about DRM maybe

        - statstical signal processing as it applies here - essnetially building a model of a system in some sort of homeostasis, figuring out when something novel happens, and shipping only .... the model .... and the deviation ....