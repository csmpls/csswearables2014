






Another approach is to trade some individual data points for a model that describes (statistically, though not precisely) how a reading appeared over a period of time. For instance, a brain with relatively stable activity (let's say, deep sleep, or REM, typified by elevated activity in the gamma band and supressed activity in the alpha band) need not be described as an hour of continuous readings; instead, it could be described as a statistical range of observed values, or even as a cyclical pattern that repeats, if such a model is relevant and appropriate. Data in this format need be represented merely by the model and whatever observed data deviated from it (e.g., "an hour of REM with a spike in alpha activity between seconds 530 and 620"). \textit{does this sort of technique have a name?}
















- data repositories for science



 


# discussion

welp good for bci and neuro BUT this is applicable to ALL sensor data we get in the wild

not only that - these devices ARE generalizeable sensors! - along with EEG data they also pick up electromuscular activity (EMG) and activity from ambient magnetic fields.








# on implementation 

## a file format

god...you know....hahahaha........but ok, we don't need ALL of the power spectra, just the `informative' or `interesting' ones.....there may be information theoretic ways of approaching the question of what is `informative' and the way i think of it (because of what nails look like to hammers) is that, if we have some model (an SVM or a clustering algo or something), then the informative bits are the parts are model can't easily explain - the thing sthat fall outside of our clusters, let's say. the rest can be described approximately but sufficiently by the model we already ha
ve. then, ok, we just need to ship the model (calibrated client-side i suppose) + the parts that don't fit into it and when they came/how they don't fit in

        - statstical signal processing as it applies here - essnetially building a model of a system in some sort of homeostasis, figuring out when something novel happens, and shipping only .... the model .... and the deviation ....

another idea is just the kind of binning we've been doing.....and compressing signals in the time dimension.......the latter in particular.......we can present evidence that doing this helps, though it's less seductive (from an academic standpoint) then whatever we might be doing with models and so on

to my mind, an open question here is what to do with those very high bins that we're sure **aren't** endogenous neural signals. of course i propose we keep them and that they're useful but how do we treat them for the purposes of this conversation? do we mention that at all?






## an earlier draft on this same topic that i certainly wont read rn lol

what are the "interesting bits" in the context of neural data?

binning & flattening in temporal dimension could ``average out'' informative aspects of the signal. Further work must investigate how these compression strategies affect analytical power, testing them on traditional BCI applications relying on motor imagery, SSVP, ERP, P300, etc.



## drm for privacy preservation

recordings should be destructable by the users that made them

optionally, require permission to access data - if someone queries your data, they need to send a request to you. handled cryptographically somehow.

## the end user

incentivize with apps

emphasize that application layer and data collection layer are fully separable

if apps require some ML, developers can "push" new models based off data collected from their users.

here we can talk about "delivering a classifier"














# specifications



we need the "mp3" of brainwave data. a lossy but acceptable format that allows these data to be easily transferred. 

## topics 

### - what that file format might look like ?

big idea here is 'replacing the db with the model'

well, compression, but what does compression mean here? what are "the interesting bits"? does it all depend on the model? or is there some generalizeable way of finding information, a la doing "diffs" on power frequency data? like, wow that frequency bin really peaked on that second, while the rest of those bins hovered around where they were?










 (1) it's not owned by someone and (2) people can revoke access to their data 

tension btwn 1 and 2



ships you back some ML metric based on cloud........constantly refactoring metric │07-21 23:24 --> going back to this idea of classifier instead of a database. │07-21 23:24 --> the cloud just ships you a classifier, rather than the data used to create it.

                      │07-21 23:25 <== you need to collect some of the data but you never need to SEND data




a system by which you need a person's "permission" to unlock the data?


I think the concept of
                       │                minimizing what is in the
                       │                database to the "interesting"
                       │                bits would be dramatically
                       │                helpful for compression
                       │07-21 23:38 --> agreed.
    7-21 23:38 <== upstream, no data matters except the data that doesn't match the model



so.........what's the model? does it all depend on the model we're using? or is there some generalizeable way of finding information, a la doing "diffs" on power frequency data? like, wow that frequency bin really peaked on that second, while the rest of those bins hovered around where they were?





take the fully academic approach


        - distributed data stores

        - stuff about DRM maybe

